{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "device = torch.device(\"cuda:0\")  \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/iorana/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/iorana/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'NUM', 'ADP', 'ADJ', 'PRON', 'PRT', 'NOUN', '.', 'ADV', 'X', 'CONJ', 'VERB', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = word_emb_dim,\n",
    "                            hidden_size = lstm_hidden_dim,\n",
    "                            num_layers = lstm_layers_count)\n",
    "        self.logits = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.logits(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "def calc_accuracy(model, data):\n",
    "    x, y = data\n",
    "    prediction = model(x)\n",
    "    mask = y > 0\n",
    "    indices = torch.argmax(prediction, dim= -1)\n",
    "    correct_count = torch.sum(indices[mask] == y[mask])\n",
    "    sum_count = torch.sum(mask)\n",
    "    return correct_count.item(), sum_count.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5627, grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 92)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss = criterion(logits.permute(1, 2, 0), y_batch.permute(1, 0))\n",
    "print(loss)\n",
    "calc_accuracy(model, (X_batch, y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.permute(1,2,0), y_batch.permute(1,0))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = calc_accuracy(model, (X_batch, y_batch))\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.68740, Accuracy = 78.52%: 100%|██████████| 572/572 [00:04<00:00, 125.63it/s]\n",
      "[1 / 50]   Val: Loss = 0.35047, Accuracy = 88.34%: 100%|██████████| 13/13 [00:00<00:00, 83.40it/s]\n",
      "[2 / 50] Train: Loss = 0.27428, Accuracy = 91.12%: 100%|██████████| 572/572 [00:04<00:00, 129.38it/s]\n",
      "[2 / 50]   Val: Loss = 0.23196, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 82.42it/s]\n",
      "[3 / 50] Train: Loss = 0.18570, Accuracy = 94.04%: 100%|██████████| 572/572 [00:04<00:00, 128.57it/s]\n",
      "[3 / 50]   Val: Loss = 0.18658, Accuracy = 93.90%: 100%|██████████| 13/13 [00:00<00:00, 79.27it/s]\n",
      "[4 / 50] Train: Loss = 0.13897, Accuracy = 95.57%: 100%|██████████| 572/572 [00:04<00:00, 133.44it/s]\n",
      "[4 / 50]   Val: Loss = 0.16468, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 84.47it/s]\n",
      "[5 / 50] Train: Loss = 0.10860, Accuracy = 96.54%: 100%|██████████| 572/572 [00:04<00:00, 133.20it/s]\n",
      "[5 / 50]   Val: Loss = 0.15443, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 82.75it/s]\n",
      "[6 / 50] Train: Loss = 0.08687, Accuracy = 97.25%: 100%|██████████| 572/572 [00:04<00:00, 132.11it/s]\n",
      "[6 / 50]   Val: Loss = 0.15100, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 80.92it/s]\n",
      "[7 / 50] Train: Loss = 0.07021, Accuracy = 97.78%: 100%|██████████| 572/572 [00:04<00:00, 127.84it/s]\n",
      "[7 / 50]   Val: Loss = 0.15635, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 82.13it/s]\n",
      "[8 / 50] Train: Loss = 0.05709, Accuracy = 98.22%: 100%|██████████| 572/572 [00:04<00:00, 128.44it/s]\n",
      "[8 / 50]   Val: Loss = 0.15574, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 81.08it/s]\n",
      "[9 / 50] Train: Loss = 0.04688, Accuracy = 98.57%: 100%|██████████| 572/572 [00:04<00:00, 129.17it/s] \n",
      "[9 / 50]   Val: Loss = 0.16177, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 85.09it/s]\n",
      "[10 / 50] Train: Loss = 0.03846, Accuracy = 98.85%: 100%|██████████| 572/572 [00:04<00:00, 130.65it/s]\n",
      "[10 / 50]   Val: Loss = 0.16992, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 87.55it/s]\n",
      "[11 / 50] Train: Loss = 0.03133, Accuracy = 99.10%: 100%|██████████| 572/572 [00:04<00:00, 129.83it/s] \n",
      "[11 / 50]   Val: Loss = 0.18182, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 79.67it/s]\n",
      "[12 / 50] Train: Loss = 0.02549, Accuracy = 99.30%: 100%|██████████| 572/572 [00:04<00:00, 131.38it/s] \n",
      "[12 / 50]   Val: Loss = 0.19024, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 82.94it/s]\n",
      "[13 / 50] Train: Loss = 0.02079, Accuracy = 99.46%: 100%|██████████| 572/572 [00:04<00:00, 129.28it/s]\n",
      "[13 / 50]   Val: Loss = 0.19992, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 78.06it/s]\n",
      "[14 / 50] Train: Loss = 0.01693, Accuracy = 99.59%: 100%|██████████| 572/572 [00:04<00:00, 130.98it/s] \n",
      "[14 / 50]   Val: Loss = 0.21677, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 79.32it/s]\n",
      "[15 / 50] Train: Loss = 0.01381, Accuracy = 99.67%: 100%|██████████| 572/572 [00:04<00:00, 130.18it/s] \n",
      "[15 / 50]   Val: Loss = 0.22781, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 79.82it/s]\n",
      "[16 / 50] Train: Loss = 0.01130, Accuracy = 99.74%: 100%|██████████| 572/572 [00:04<00:00, 130.96it/s] \n",
      "[16 / 50]   Val: Loss = 0.23672, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 78.60it/s]\n",
      "[17 / 50] Train: Loss = 0.00938, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 133.04it/s] \n",
      "[17 / 50]   Val: Loss = 0.24823, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 78.68it/s]\n",
      "[18 / 50] Train: Loss = 0.00813, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 129.22it/s] \n",
      "[18 / 50]   Val: Loss = 0.26162, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 82.78it/s]\n",
      "[19 / 50] Train: Loss = 0.00713, Accuracy = 99.83%: 100%|██████████| 572/572 [00:04<00:00, 129.57it/s] \n",
      "[19 / 50]   Val: Loss = 0.27185, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 79.82it/s]\n",
      "[20 / 50] Train: Loss = 0.00657, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 132.83it/s] \n",
      "[20 / 50]   Val: Loss = 0.27546, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 81.49it/s]\n",
      "[21 / 50] Train: Loss = 0.00647, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 128.70it/s] \n",
      "[21 / 50]   Val: Loss = 0.28889, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 79.21it/s]\n",
      "[22 / 50] Train: Loss = 0.00600, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.76it/s] \n",
      "[22 / 50]   Val: Loss = 0.28879, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 83.59it/s]\n",
      "[23 / 50] Train: Loss = 0.00570, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.69it/s] \n",
      "[23 / 50]   Val: Loss = 0.29997, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 79.00it/s]\n",
      "[24 / 50] Train: Loss = 0.00546, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.41it/s] \n",
      "[24 / 50]   Val: Loss = 0.29930, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 83.84it/s]\n",
      "[25 / 50] Train: Loss = 0.00514, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.83it/s] \n",
      "[25 / 50]   Val: Loss = 0.30756, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 81.64it/s]\n",
      "[26 / 50] Train: Loss = 0.00495, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.43it/s] \n",
      "[26 / 50]   Val: Loss = 0.31424, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 80.95it/s]\n",
      "[27 / 50] Train: Loss = 0.00494, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 134.52it/s] \n",
      "[27 / 50]   Val: Loss = 0.31503, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 81.79it/s]\n",
      "[28 / 50] Train: Loss = 0.00531, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 129.34it/s] \n",
      "[28 / 50]   Val: Loss = 0.31885, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 78.50it/s]\n",
      "[29 / 50] Train: Loss = 0.00511, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.43it/s] \n",
      "[29 / 50]   Val: Loss = 0.32634, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 81.80it/s]\n",
      "[30 / 50] Train: Loss = 0.00479, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.44it/s] \n",
      "[30 / 50]   Val: Loss = 0.32459, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 80.82it/s]\n",
      "[31 / 50] Train: Loss = 0.00428, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.24it/s] \n",
      "[31 / 50]   Val: Loss = 0.33285, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 82.14it/s]\n",
      "[32 / 50] Train: Loss = 0.00427, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.55it/s] \n",
      "[32 / 50]   Val: Loss = 0.34097, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 83.00it/s]\n",
      "[33 / 50] Train: Loss = 0.00417, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 127.74it/s] \n",
      "[33 / 50]   Val: Loss = 0.33706, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 82.50it/s]\n",
      "[34 / 50] Train: Loss = 0.00425, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 123.79it/s] \n",
      "[34 / 50]   Val: Loss = 0.33631, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 81.60it/s]\n",
      "[35 / 50] Train: Loss = 0.00608, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 128.22it/s] \n",
      "[35 / 50]   Val: Loss = 0.33788, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 78.76it/s]\n",
      "[36 / 50] Train: Loss = 0.00506, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 138.14it/s] \n",
      "[36 / 50]   Val: Loss = 0.34281, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 84.21it/s]\n",
      "[37 / 50] Train: Loss = 0.00414, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 131.79it/s] \n",
      "[37 / 50]   Val: Loss = 0.34246, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 83.85it/s]\n",
      "[38 / 50] Train: Loss = 0.00392, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 126.49it/s] \n",
      "[38 / 50]   Val: Loss = 0.34149, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 77.57it/s]\n",
      "[39 / 50] Train: Loss = 0.00385, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 128.46it/s] \n",
      "[39 / 50]   Val: Loss = 0.34617, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 82.90it/s]\n",
      "[40 / 50] Train: Loss = 0.00393, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.89it/s] \n",
      "[40 / 50]   Val: Loss = 0.34350, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 79.54it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.00398, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 131.05it/s] \n",
      "[41 / 50]   Val: Loss = 0.34550, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 78.22it/s]\n",
      "[42 / 50] Train: Loss = 0.00635, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.30it/s] \n",
      "[42 / 50]   Val: Loss = 0.34083, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 84.47it/s]\n",
      "[43 / 50] Train: Loss = 0.00475, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 128.47it/s] \n",
      "[43 / 50]   Val: Loss = 0.34001, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 82.83it/s]\n",
      "[44 / 50] Train: Loss = 0.00391, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 127.34it/s] \n",
      "[44 / 50]   Val: Loss = 0.34322, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 81.81it/s]\n",
      "[45 / 50] Train: Loss = 0.00369, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.07it/s] \n",
      "[45 / 50]   Val: Loss = 0.34402, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 82.54it/s]\n",
      "[46 / 50] Train: Loss = 0.00370, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 130.43it/s] \n",
      "[46 / 50]   Val: Loss = 0.35061, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 84.14it/s]\n",
      "[47 / 50] Train: Loss = 0.00380, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 138.56it/s] \n",
      "[47 / 50]   Val: Loss = 0.35052, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 82.19it/s]\n",
      "[48 / 50] Train: Loss = 0.00386, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 131.13it/s] \n",
      "[48 / 50]   Val: Loss = 0.35473, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 83.40it/s]\n",
      "[49 / 50] Train: Loss = 0.00555, Accuracy = 99.85%: 100%|██████████| 572/572 [00:04<00:00, 129.85it/s] \n",
      "[49 / 50]   Val: Loss = 0.34393, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 82.92it/s]\n",
      "[50 / 50] Train: Loss = 0.00459, Accuracy = 99.86%: 100%|██████████| 572/572 [00:04<00:00, 130.90it/s] \n",
      "[50 / 50]   Val: Loss = 0.35071, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 80.62it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind),\n",
    "    word_emb_dim=100,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_layers_count=1\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": [
    "def check_accuracy_on_test_data(model):\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    model.eval()\n",
    "\n",
    "    test_batch_size = 512\n",
    "\n",
    "    data = (X_test, y_test)\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, test_batch_size)):\n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "        cur_correct_count, cur_sum_count = calc_accuracy(model, (X_batch, y_batch))\n",
    "        correct_count += cur_correct_count\n",
    "        sum_count += cur_sum_count\n",
    "    print('Accuracy on test data: {:.2%}'.format(correct_count/sum_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 95.36%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy_on_test_data(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = word_emb_dim,\n",
    "                            hidden_size = lstm_hidden_dim,\n",
    "                            num_layers = lstm_layers_count,\n",
    "                            bidirectional=True)\n",
    "        self.logits = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.logits(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.55826, Accuracy = 82.88%: 100%|██████████| 572/572 [00:06<00:00, 91.25it/s]\n",
      "[1 / 50]   Val: Loss = 0.27132, Accuracy = 91.28%: 100%|██████████| 13/13 [00:00<00:00, 61.25it/s]\n",
      "[2 / 50] Train: Loss = 0.20537, Accuracy = 93.68%: 100%|██████████| 572/572 [00:06<00:00, 89.86it/s]\n",
      "[2 / 50]   Val: Loss = 0.17925, Accuracy = 94.39%: 100%|██████████| 13/13 [00:00<00:00, 59.57it/s]\n",
      "[3 / 50] Train: Loss = 0.12998, Accuracy = 96.20%: 100%|██████████| 572/572 [00:06<00:00, 94.55it/s]\n",
      "[3 / 50]   Val: Loss = 0.13946, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 61.02it/s]\n",
      "[4 / 50] Train: Loss = 0.08859, Accuracy = 97.50%: 100%|██████████| 572/572 [00:06<00:00, 91.69it/s]\n",
      "[4 / 50]   Val: Loss = 0.12423, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 60.55it/s]\n",
      "[5 / 50] Train: Loss = 0.06090, Accuracy = 98.37%: 100%|██████████| 572/572 [00:06<00:00, 92.20it/s]\n",
      "[5 / 50]   Val: Loss = 0.11788, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 60.87it/s]\n",
      "[6 / 50] Train: Loss = 0.04135, Accuracy = 98.97%: 100%|██████████| 572/572 [00:06<00:00, 92.10it/s]\n",
      "[6 / 50]   Val: Loss = 0.11740, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 60.03it/s]\n",
      "[7 / 50] Train: Loss = 0.02753, Accuracy = 99.37%: 100%|██████████| 572/572 [00:06<00:00, 91.12it/s]\n",
      "[7 / 50]   Val: Loss = 0.12481, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 59.39it/s]\n",
      "[8 / 50] Train: Loss = 0.01793, Accuracy = 99.65%: 100%|██████████| 572/572 [00:06<00:00, 91.66it/s] \n",
      "[8 / 50]   Val: Loss = 0.13095, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 57.59it/s]\n",
      "[9 / 50] Train: Loss = 0.01138, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 93.52it/s] \n",
      "[9 / 50]   Val: Loss = 0.13762, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 60.03it/s]\n",
      "[10 / 50] Train: Loss = 0.00703, Accuracy = 99.91%: 100%|██████████| 572/572 [00:06<00:00, 90.96it/s] \n",
      "[10 / 50]   Val: Loss = 0.14396, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 58.41it/s]\n",
      "[11 / 50] Train: Loss = 0.00432, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 90.79it/s] \n",
      "[11 / 50]   Val: Loss = 0.15683, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 57.25it/s]\n",
      "[12 / 50] Train: Loss = 0.00274, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 91.47it/s]  \n",
      "[12 / 50]   Val: Loss = 0.16008, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 62.23it/s]\n",
      "[13 / 50] Train: Loss = 0.00177, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 90.85it/s] \n",
      "[13 / 50]   Val: Loss = 0.16831, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 56.78it/s]\n",
      "[14 / 50] Train: Loss = 0.00120, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 88.38it/s] \n",
      "[14 / 50]   Val: Loss = 0.17950, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 59.45it/s]\n",
      "[15 / 50] Train: Loss = 0.00121, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.86it/s]\n",
      "[15 / 50]   Val: Loss = 0.18941, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 59.05it/s]\n",
      "[16 / 50] Train: Loss = 0.00426, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 96.23it/s]  \n",
      "[16 / 50]   Val: Loss = 0.19109, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 58.45it/s]\n",
      "[17 / 50] Train: Loss = 0.00163, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 96.36it/s] \n",
      "[17 / 50]   Val: Loss = 0.19899, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 57.67it/s]\n",
      "[18 / 50] Train: Loss = 0.00053, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 99.00it/s] \n",
      "[18 / 50]   Val: Loss = 0.19792, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 58.22it/s]\n",
      "[19 / 50] Train: Loss = 0.00030, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 94.93it/s] \n",
      "[19 / 50]   Val: Loss = 0.20039, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 57.41it/s]\n",
      "[20 / 50] Train: Loss = 0.00019, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.79it/s] \n",
      "[20 / 50]   Val: Loss = 0.20425, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 62.97it/s]\n",
      "[21 / 50] Train: Loss = 0.00019, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.62it/s]\n",
      "[21 / 50]   Val: Loss = 0.20836, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 60.55it/s]\n",
      "[22 / 50] Train: Loss = 0.00391, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 92.55it/s]  \n",
      "[22 / 50]   Val: Loss = 0.20652, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 59.63it/s]\n",
      "[23 / 50] Train: Loss = 0.00184, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 92.13it/s]  \n",
      "[23 / 50]   Val: Loss = 0.20628, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 55.12it/s]\n",
      "[24 / 50] Train: Loss = 0.00048, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.85it/s]\n",
      "[24 / 50]   Val: Loss = 0.20797, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 56.24it/s]\n",
      "[25 / 50] Train: Loss = 0.00016, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.70it/s]\n",
      "[25 / 50]   Val: Loss = 0.21043, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 58.31it/s]\n",
      "[26 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.35it/s]\n",
      "[26 / 50]   Val: Loss = 0.21174, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 58.90it/s]\n",
      "[27 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.82it/s]\n",
      "[27 / 50]   Val: Loss = 0.21480, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 59.94it/s]\n",
      "[28 / 50] Train: Loss = 0.00016, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.69it/s]\n",
      "[28 / 50]   Val: Loss = 0.21756, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 59.86it/s]\n",
      "[29 / 50] Train: Loss = 0.00403, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 91.93it/s] \n",
      "[29 / 50]   Val: Loss = 0.22466, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 57.79it/s]\n",
      "[30 / 50] Train: Loss = 0.00111, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.54it/s]\n",
      "[30 / 50]   Val: Loss = 0.22668, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 58.72it/s]\n",
      "[31 / 50] Train: Loss = 0.00024, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 92.64it/s]\n",
      "[31 / 50]   Val: Loss = 0.22378, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 61.57it/s]\n",
      "[32 / 50] Train: Loss = 0.00010, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 92.55it/s]\n",
      "[32 / 50]   Val: Loss = 0.22809, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 61.37it/s]\n",
      "[33 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 93.83it/s] \n",
      "[33 / 50]   Val: Loss = 0.22947, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 57.54it/s]\n",
      "[34 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.05it/s]\n",
      "[34 / 50]   Val: Loss = 0.23058, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 59.46it/s]\n",
      "[35 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.09it/s]\n",
      "[35 / 50]   Val: Loss = 0.23180, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 58.30it/s]\n",
      "[36 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.75it/s] \n",
      "[36 / 50]   Val: Loss = 0.23427, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 55.22it/s]\n",
      "[37 / 50] Train: Loss = 0.00393, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 89.14it/s] \n",
      "[37 / 50]   Val: Loss = 0.24171, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 58.35it/s]\n",
      "[38 / 50] Train: Loss = 0.00122, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.82it/s]\n",
      "[38 / 50]   Val: Loss = 0.23320, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 60.78it/s]\n",
      "[39 / 50] Train: Loss = 0.00020, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.12it/s]\n",
      "[39 / 50]   Val: Loss = 0.23568, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 60.16it/s]\n",
      "[40 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.76it/s]\n",
      "[40 / 50]   Val: Loss = 0.23679, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 55.74it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.63it/s]\n",
      "[41 / 50]   Val: Loss = 0.23828, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 57.69it/s]\n",
      "[42 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.91it/s]\n",
      "[42 / 50]   Val: Loss = 0.23912, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 59.97it/s]\n",
      "[43 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.53it/s]\n",
      "[43 / 50]   Val: Loss = 0.24211, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 59.05it/s]\n",
      "[44 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.88it/s]\n",
      "[44 / 50]   Val: Loss = 0.24173, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 60.95it/s]\n",
      "[45 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 92.38it/s]\n",
      "[45 / 50]   Val: Loss = 0.24435, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 61.04it/s]\n",
      "[46 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.14it/s]\n",
      "[46 / 50]   Val: Loss = 0.24586, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 58.74it/s]\n",
      "[47 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.57it/s]\n",
      "[47 / 50]   Val: Loss = 0.24815, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 61.26it/s]\n",
      "[48 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.53it/s]\n",
      "[48 / 50]   Val: Loss = 0.25088, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 58.38it/s]\n",
      "[49 / 50] Train: Loss = 0.00318, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 89.69it/s] \n",
      "[49 / 50]   Val: Loss = 0.24799, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 59.47it/s]\n",
      "[50 / 50] Train: Loss = 0.00234, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 93.40it/s]  \n",
      "[50 / 50]   Val: Loss = 0.24133, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 61.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind),\n",
    "    word_emb_dim=100,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_layers_count=1\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 96.87%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy_on_test_data(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embeddings), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = 100,\n",
    "                            hidden_size = lstm_hidden_dim,\n",
    "                            num_layers = lstm_layers_count,\n",
    "                            bidirectional=True)\n",
    "        self.logits = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.logits(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 13] Train: Loss = 0.53802, Accuracy = 83.87%: 100%|██████████| 572/572 [00:10<00:00, 54.83it/s]\n",
      "[1 / 13]   Val: Loss = 0.19900, Accuracy = 93.97%: 100%|██████████| 13/13 [00:00<00:00, 55.36it/s]\n",
      "[2 / 13] Train: Loss = 0.13384, Accuracy = 96.07%: 100%|██████████| 572/572 [00:10<00:00, 54.45it/s]\n",
      "[2 / 13]   Val: Loss = 0.12923, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 60.06it/s]\n",
      "[3 / 13] Train: Loss = 0.09564, Accuracy = 97.15%: 100%|██████████| 572/572 [00:10<00:00, 54.78it/s]\n",
      "[3 / 13]   Val: Loss = 0.10976, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 55.01it/s]\n",
      "[4 / 13] Train: Loss = 0.07929, Accuracy = 97.61%: 100%|██████████| 572/572 [00:09<00:00, 57.27it/s] \n",
      "[4 / 13]   Val: Loss = 0.10054, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 56.65it/s]\n",
      "[5 / 13] Train: Loss = 0.06960, Accuracy = 97.90%: 100%|██████████| 572/572 [00:10<00:00, 54.81it/s]\n",
      "[5 / 13]   Val: Loss = 0.09963, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 53.98it/s]\n",
      "[6 / 13] Train: Loss = 0.06280, Accuracy = 98.12%: 100%|██████████| 572/572 [00:10<00:00, 53.95it/s] \n",
      "[6 / 13]   Val: Loss = 0.09401, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 56.84it/s]\n",
      "[7 / 13] Train: Loss = 0.05745, Accuracy = 98.26%: 100%|██████████| 572/572 [00:10<00:00, 55.06it/s]\n",
      "[7 / 13]   Val: Loss = 0.09305, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 57.85it/s]\n",
      "[8 / 13] Train: Loss = 0.05289, Accuracy = 98.40%: 100%|██████████| 572/572 [00:10<00:00, 55.29it/s]\n",
      "[8 / 13]   Val: Loss = 0.08882, Accuracy = 97.16%: 100%|██████████| 13/13 [00:00<00:00, 58.56it/s]\n",
      "[9 / 13] Train: Loss = 0.04860, Accuracy = 98.53%: 100%|██████████| 572/572 [00:10<00:00, 54.13it/s]\n",
      "[9 / 13]   Val: Loss = 0.09264, Accuracy = 97.15%: 100%|██████████| 13/13 [00:00<00:00, 53.24it/s]\n",
      "[10 / 13] Train: Loss = 0.04543, Accuracy = 98.64%: 100%|██████████| 572/572 [00:10<00:00, 54.09it/s]\n",
      "[10 / 13]   Val: Loss = 0.08962, Accuracy = 97.18%: 100%|██████████| 13/13 [00:00<00:00, 55.07it/s]\n",
      "[11 / 13] Train: Loss = 0.04240, Accuracy = 98.74%: 100%|██████████| 572/572 [00:10<00:00, 54.29it/s]\n",
      "[11 / 13]   Val: Loss = 0.09284, Accuracy = 97.10%: 100%|██████████| 13/13 [00:00<00:00, 53.86it/s]\n",
      "[12 / 13] Train: Loss = 0.03961, Accuracy = 98.83%: 100%|██████████| 572/572 [00:10<00:00, 51.66it/s]\n",
      "[12 / 13]   Val: Loss = 0.09123, Accuracy = 97.15%: 100%|██████████| 13/13 [00:00<00:00, 55.76it/s]\n",
      "[13 / 13] Train: Loss = 0.03655, Accuracy = 98.93%: 100%|██████████| 572/572 [00:10<00:00, 54.01it/s]\n",
      "[13 / 13]   Val: Loss = 0.09385, Accuracy = 97.17%: 100%|██████████| 13/13 [00:00<00:00, 52.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind),\n",
    "    lstm_hidden_dim=64,\n",
    "    lstm_layers_count=2\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=13,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 97.17%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy_on_test_data(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
