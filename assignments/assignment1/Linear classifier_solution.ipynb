{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics_solution import multiclass_accuracy \n",
    "import linear_classifer_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer_solution.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer_solution.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer_solution.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer_solution.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer_solution.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer_solution.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer_solution.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer_solution.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer_solution.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer_solution.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.437266\n",
      "Epoch 1, loss: 2.428577\n",
      "Epoch 2, loss: 2.451567\n",
      "Epoch 3, loss: 2.472476\n",
      "Epoch 4, loss: 2.486346\n",
      "Epoch 5, loss: 2.494643\n",
      "Epoch 6, loss: 2.499315\n",
      "Epoch 7, loss: 2.501980\n",
      "Epoch 8, loss: 2.503445\n",
      "Epoch 9, loss: 2.504394\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer_solution.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff90ab239d0>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.088\n",
      "Epoch 0, loss: 2.504693\n",
      "Epoch 1, loss: 2.504904\n",
      "Epoch 2, loss: 2.505113\n",
      "Epoch 3, loss: 2.505312\n",
      "Epoch 4, loss: 2.505502\n",
      "Epoch 5, loss: 2.505235\n",
      "Epoch 6, loss: 2.505232\n",
      "Epoch 7, loss: 2.505289\n",
      "Epoch 8, loss: 2.505308\n",
      "Epoch 9, loss: 2.505184\n",
      "Epoch 10, loss: 2.505391\n",
      "Epoch 11, loss: 2.505189\n",
      "Epoch 12, loss: 2.505256\n",
      "Epoch 13, loss: 2.505188\n",
      "Epoch 14, loss: 2.505245\n",
      "Epoch 15, loss: 2.505447\n",
      "Epoch 16, loss: 2.505562\n",
      "Epoch 17, loss: 2.505701\n",
      "Epoch 18, loss: 2.505254\n",
      "Epoch 19, loss: 2.505332\n",
      "Epoch 20, loss: 2.505201\n",
      "Epoch 21, loss: 2.505375\n",
      "Epoch 22, loss: 2.505511\n",
      "Epoch 23, loss: 2.505255\n",
      "Epoch 24, loss: 2.505206\n",
      "Epoch 25, loss: 2.505346\n",
      "Epoch 26, loss: 2.505309\n",
      "Epoch 27, loss: 2.505441\n",
      "Epoch 28, loss: 2.505331\n",
      "Epoch 29, loss: 2.505186\n",
      "Epoch 30, loss: 2.505211\n",
      "Epoch 31, loss: 2.505201\n",
      "Epoch 32, loss: 2.505183\n",
      "Epoch 33, loss: 2.505219\n",
      "Epoch 34, loss: 2.505372\n",
      "Epoch 35, loss: 2.505236\n",
      "Epoch 36, loss: 2.505225\n",
      "Epoch 37, loss: 2.505196\n",
      "Epoch 38, loss: 2.505313\n",
      "Epoch 39, loss: 2.505276\n",
      "Epoch 40, loss: 2.505327\n",
      "Epoch 41, loss: 2.505325\n",
      "Epoch 42, loss: 2.505404\n",
      "Epoch 43, loss: 2.505253\n",
      "Epoch 44, loss: 2.505535\n",
      "Epoch 45, loss: 2.505241\n",
      "Epoch 46, loss: 2.505320\n",
      "Epoch 47, loss: 2.505207\n",
      "Epoch 48, loss: 2.505217\n",
      "Epoch 49, loss: 2.505252\n",
      "Epoch 50, loss: 2.505219\n",
      "Epoch 51, loss: 2.505216\n",
      "Epoch 52, loss: 2.505240\n",
      "Epoch 53, loss: 2.505383\n",
      "Epoch 54, loss: 2.505339\n",
      "Epoch 55, loss: 2.505240\n",
      "Epoch 56, loss: 2.505303\n",
      "Epoch 57, loss: 2.505206\n",
      "Epoch 58, loss: 2.505230\n",
      "Epoch 59, loss: 2.505239\n",
      "Epoch 60, loss: 2.505341\n",
      "Epoch 61, loss: 2.505205\n",
      "Epoch 62, loss: 2.505454\n",
      "Epoch 63, loss: 2.505256\n",
      "Epoch 64, loss: 2.505223\n",
      "Epoch 65, loss: 2.505225\n",
      "Epoch 66, loss: 2.505226\n",
      "Epoch 67, loss: 2.505495\n",
      "Epoch 68, loss: 2.505306\n",
      "Epoch 69, loss: 2.505297\n",
      "Epoch 70, loss: 2.505193\n",
      "Epoch 71, loss: 2.505379\n",
      "Epoch 72, loss: 2.505236\n",
      "Epoch 73, loss: 2.505286\n",
      "Epoch 74, loss: 2.505252\n",
      "Epoch 75, loss: 2.505406\n",
      "Epoch 76, loss: 2.505286\n",
      "Epoch 77, loss: 2.505334\n",
      "Epoch 78, loss: 2.505404\n",
      "Epoch 79, loss: 2.505214\n",
      "Epoch 80, loss: 2.505244\n",
      "Epoch 81, loss: 2.505169\n",
      "Epoch 82, loss: 2.505251\n",
      "Epoch 83, loss: 2.505183\n",
      "Epoch 84, loss: 2.505186\n",
      "Epoch 85, loss: 2.505223\n",
      "Epoch 86, loss: 2.505191\n",
      "Epoch 87, loss: 2.505200\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
