{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics_solution import multiclass_accuracy \n",
    "import linear_classifer_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer_solution.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer_solution.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer_solution.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer_solution.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer_solution.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer_solution.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer_solution.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer_solution.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer_solution.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer_solution.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.439188\n",
      "Epoch 1, loss: 2.430187\n",
      "Epoch 2, loss: 2.452619\n",
      "Epoch 3, loss: 2.473063\n",
      "Epoch 4, loss: 2.486676\n",
      "Epoch 5, loss: 2.494781\n",
      "Epoch 6, loss: 2.499508\n",
      "Epoch 7, loss: 2.502128\n",
      "Epoch 8, loss: 2.503535\n",
      "Epoch 9, loss: 2.504378\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer_solution.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1bb9fedad0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.102\n",
      "Epoch 0, loss: 2.504916\n",
      "Epoch 1, loss: 2.504989\n",
      "Epoch 2, loss: 2.505074\n",
      "Epoch 3, loss: 2.505153\n",
      "Epoch 4, loss: 2.505317\n",
      "Epoch 5, loss: 2.505212\n",
      "Epoch 6, loss: 2.505201\n",
      "Epoch 7, loss: 2.505266\n",
      "Epoch 8, loss: 2.505222\n",
      "Epoch 9, loss: 2.505202\n",
      "Epoch 10, loss: 2.505175\n",
      "Epoch 11, loss: 2.505379\n",
      "Epoch 12, loss: 2.505632\n",
      "Epoch 13, loss: 2.505502\n",
      "Epoch 14, loss: 2.505378\n",
      "Epoch 15, loss: 2.505376\n",
      "Epoch 16, loss: 2.505383\n",
      "Epoch 17, loss: 2.505353\n",
      "Epoch 18, loss: 2.505440\n",
      "Epoch 19, loss: 2.505376\n",
      "Epoch 20, loss: 2.505433\n",
      "Epoch 21, loss: 2.505273\n",
      "Epoch 22, loss: 2.505241\n",
      "Epoch 23, loss: 2.505457\n",
      "Epoch 24, loss: 2.505395\n",
      "Epoch 25, loss: 2.505438\n",
      "Epoch 26, loss: 2.505196\n",
      "Epoch 27, loss: 2.505234\n",
      "Epoch 28, loss: 2.505180\n",
      "Epoch 29, loss: 2.505214\n",
      "Epoch 30, loss: 2.505165\n",
      "Epoch 31, loss: 2.505291\n",
      "Epoch 32, loss: 2.505265\n",
      "Epoch 33, loss: 2.505211\n",
      "Epoch 34, loss: 2.505191\n",
      "Epoch 35, loss: 2.505242\n",
      "Epoch 36, loss: 2.505208\n",
      "Epoch 37, loss: 2.505535\n",
      "Epoch 38, loss: 2.505435\n",
      "Epoch 39, loss: 2.505614\n",
      "Epoch 40, loss: 2.505232\n",
      "Epoch 41, loss: 2.505295\n",
      "Epoch 42, loss: 2.505241\n",
      "Epoch 43, loss: 2.505261\n",
      "Epoch 44, loss: 2.505186\n",
      "Epoch 45, loss: 2.505267\n",
      "Epoch 46, loss: 2.505371\n",
      "Epoch 47, loss: 2.505235\n",
      "Epoch 48, loss: 2.505206\n",
      "Epoch 49, loss: 2.505265\n",
      "Epoch 50, loss: 2.505355\n",
      "Epoch 51, loss: 2.505291\n",
      "Epoch 52, loss: 2.505345\n",
      "Epoch 53, loss: 2.505222\n",
      "Epoch 54, loss: 2.505187\n",
      "Epoch 55, loss: 2.505503\n",
      "Epoch 56, loss: 2.505344\n",
      "Epoch 57, loss: 2.505528\n",
      "Epoch 58, loss: 2.505295\n",
      "Epoch 59, loss: 2.505416\n",
      "Epoch 60, loss: 2.505357\n",
      "Epoch 61, loss: 2.505288\n",
      "Epoch 62, loss: 2.505278\n",
      "Epoch 63, loss: 2.505215\n",
      "Epoch 64, loss: 2.505361\n",
      "Epoch 65, loss: 2.505353\n",
      "Epoch 66, loss: 2.505470\n",
      "Epoch 67, loss: 2.505231\n",
      "Epoch 68, loss: 2.505311\n",
      "Epoch 69, loss: 2.505237\n",
      "Epoch 70, loss: 2.505440\n",
      "Epoch 71, loss: 2.505292\n",
      "Epoch 72, loss: 2.505386\n",
      "Epoch 73, loss: 2.505420\n",
      "Epoch 74, loss: 2.505371\n",
      "Epoch 75, loss: 2.505268\n",
      "Epoch 76, loss: 2.505263\n",
      "Epoch 77, loss: 2.505267\n",
      "Epoch 78, loss: 2.505312\n",
      "Epoch 79, loss: 2.505185\n",
      "Epoch 80, loss: 2.505228\n",
      "Epoch 81, loss: 2.505210\n",
      "Epoch 82, loss: 2.505224\n",
      "Epoch 83, loss: 2.505221\n",
      "Epoch 84, loss: 2.505196\n",
      "Epoch 85, loss: 2.505219\n",
      "Epoch 86, loss: 2.505209\n",
      "Epoch 87, loss: 2.505353\n",
      "Epoch 88, loss: 2.505305\n",
      "Epoch 89, loss: 2.505452\n",
      "Epoch 90, loss: 2.505187\n",
      "Epoch 91, loss: 2.505394\n",
      "Epoch 92, loss: 2.505319\n",
      "Epoch 93, loss: 2.505303\n",
      "Epoch 94, loss: 2.505201\n",
      "Epoch 95, loss: 2.505371\n",
      "Epoch 96, loss: 2.505541\n",
      "Epoch 97, loss: 2.505299\n",
      "Epoch 98, loss: 2.505177\n",
      "Epoch 99, loss: 2.505188\n",
      "Accuracy after training for 100 epochs:  0.062\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302590\n",
      "Epoch 1, loss: 2.302595\n",
      "Epoch 2, loss: 2.302601\n",
      "Epoch 3, loss: 2.302608\n",
      "Epoch 4, loss: 2.302617\n",
      "Epoch 5, loss: 2.302628\n",
      "Epoch 6, loss: 2.302639\n",
      "Epoch 7, loss: 2.302653\n",
      "Epoch 8, loss: 2.302668\n",
      "Epoch 9, loss: 2.302684\n",
      "Epoch 10, loss: 2.302702\n",
      "Epoch 11, loss: 2.302721\n",
      "Epoch 12, loss: 2.302741\n",
      "Epoch 13, loss: 2.302763\n",
      "Epoch 14, loss: 2.302787\n",
      "Epoch 15, loss: 2.302812\n",
      "Epoch 16, loss: 2.302838\n",
      "Epoch 17, loss: 2.302866\n",
      "Epoch 18, loss: 2.302896\n",
      "Epoch 19, loss: 2.302926\n",
      "Epoch 20, loss: 2.302959\n",
      "Epoch 21, loss: 2.302992\n",
      "Epoch 22, loss: 2.303028\n",
      "Epoch 23, loss: 2.303064\n",
      "Epoch 24, loss: 2.303102\n",
      "Epoch 25, loss: 2.303142\n",
      "Epoch 26, loss: 2.303183\n",
      "Epoch 27, loss: 2.303226\n",
      "Epoch 28, loss: 2.303270\n",
      "Epoch 29, loss: 2.303315\n",
      "Epoch 30, loss: 2.303362\n",
      "Epoch 31, loss: 2.303410\n",
      "Epoch 32, loss: 2.303460\n",
      "Epoch 33, loss: 2.303511\n",
      "Epoch 34, loss: 2.303564\n",
      "Epoch 35, loss: 2.303618\n",
      "Epoch 36, loss: 2.303674\n",
      "Epoch 37, loss: 2.303731\n",
      "Epoch 38, loss: 2.303789\n",
      "Epoch 39, loss: 2.303849\n",
      "Epoch 40, loss: 2.303911\n",
      "Epoch 41, loss: 2.303974\n",
      "Epoch 42, loss: 2.304038\n",
      "Epoch 43, loss: 2.304104\n",
      "Epoch 44, loss: 2.304171\n",
      "Epoch 45, loss: 2.304240\n",
      "Epoch 46, loss: 2.304310\n",
      "Epoch 47, loss: 2.304382\n",
      "Epoch 48, loss: 2.304455\n",
      "Epoch 49, loss: 2.304529\n",
      "Epoch 50, loss: 2.304605\n",
      "Epoch 51, loss: 2.304683\n",
      "Epoch 52, loss: 2.304762\n",
      "Epoch 53, loss: 2.304842\n",
      "Epoch 54, loss: 2.304924\n",
      "Epoch 55, loss: 2.305007\n",
      "Epoch 56, loss: 2.305092\n",
      "Epoch 57, loss: 2.305178\n",
      "Epoch 58, loss: 2.305266\n",
      "Epoch 59, loss: 2.305355\n",
      "Epoch 60, loss: 2.305446\n",
      "Epoch 61, loss: 2.305538\n",
      "Epoch 62, loss: 2.305631\n",
      "Epoch 63, loss: 2.305726\n",
      "Epoch 64, loss: 2.305823\n",
      "Epoch 65, loss: 2.305921\n",
      "Epoch 66, loss: 2.306020\n",
      "Epoch 67, loss: 2.306121\n",
      "Epoch 68, loss: 2.306223\n",
      "Epoch 69, loss: 2.306327\n",
      "Epoch 70, loss: 2.306432\n",
      "Epoch 71, loss: 2.306539\n",
      "Epoch 72, loss: 2.306647\n",
      "Epoch 73, loss: 2.306756\n",
      "Epoch 74, loss: 2.306867\n",
      "Epoch 75, loss: 2.306980\n",
      "Epoch 76, loss: 2.307094\n",
      "Epoch 77, loss: 2.307209\n",
      "Epoch 78, loss: 2.307326\n",
      "Epoch 79, loss: 2.307444\n",
      "Epoch 80, loss: 2.307564\n",
      "Epoch 81, loss: 2.307685\n",
      "Epoch 82, loss: 2.307808\n",
      "Epoch 83, loss: 2.307932\n",
      "Epoch 84, loss: 2.308057\n",
      "Epoch 85, loss: 2.308184\n",
      "Epoch 86, loss: 2.308313\n",
      "Epoch 87, loss: 2.308443\n",
      "Epoch 88, loss: 2.308574\n",
      "Epoch 89, loss: 2.308707\n",
      "Epoch 90, loss: 2.308842\n",
      "Epoch 91, loss: 2.308977\n",
      "Epoch 92, loss: 2.309115\n",
      "Epoch 93, loss: 2.309253\n",
      "Epoch 94, loss: 2.309393\n",
      "Epoch 95, loss: 2.309535\n",
      "Epoch 96, loss: 2.309678\n",
      "Epoch 97, loss: 2.309822\n",
      "Epoch 98, loss: 2.309968\n",
      "Epoch 99, loss: 2.310116\n",
      "Accuracy after training for 100 epochs:  0.05\n",
      "Epoch 0, loss: 2.303353\n",
      "Epoch 1, loss: 2.303368\n",
      "Epoch 2, loss: 2.303383\n",
      "Epoch 3, loss: 2.303399\n",
      "Epoch 4, loss: 2.303414\n",
      "Epoch 5, loss: 2.303430\n",
      "Epoch 6, loss: 2.303445\n",
      "Epoch 7, loss: 2.303461\n",
      "Epoch 8, loss: 2.303477\n",
      "Epoch 9, loss: 2.303494\n",
      "Epoch 10, loss: 2.303510\n",
      "Epoch 11, loss: 2.303527\n",
      "Epoch 12, loss: 2.303543\n",
      "Epoch 13, loss: 2.303560\n",
      "Epoch 14, loss: 2.303577\n",
      "Epoch 15, loss: 2.303594\n",
      "Epoch 16, loss: 2.303611\n",
      "Epoch 17, loss: 2.303629\n",
      "Epoch 18, loss: 2.303646\n",
      "Epoch 19, loss: 2.303664\n",
      "Epoch 20, loss: 2.303682\n",
      "Epoch 21, loss: 2.303700\n",
      "Epoch 22, loss: 2.303718\n",
      "Epoch 23, loss: 2.303736\n",
      "Epoch 24, loss: 2.303754\n",
      "Epoch 25, loss: 2.303773\n",
      "Epoch 26, loss: 2.303792\n",
      "Epoch 27, loss: 2.303810\n",
      "Epoch 28, loss: 2.303829\n",
      "Epoch 29, loss: 2.303848\n",
      "Epoch 30, loss: 2.303868\n",
      "Epoch 31, loss: 2.303887\n",
      "Epoch 32, loss: 2.303907\n",
      "Epoch 33, loss: 2.303926\n",
      "Epoch 34, loss: 2.303946\n",
      "Epoch 35, loss: 2.303966\n",
      "Epoch 36, loss: 2.303986\n",
      "Epoch 37, loss: 2.304007\n",
      "Epoch 38, loss: 2.304027\n",
      "Epoch 39, loss: 2.304048\n",
      "Epoch 40, loss: 2.304069\n",
      "Epoch 41, loss: 2.304089\n",
      "Epoch 42, loss: 2.304110\n",
      "Epoch 43, loss: 2.304132\n",
      "Epoch 44, loss: 2.304153\n",
      "Epoch 45, loss: 2.304174\n",
      "Epoch 46, loss: 2.304196\n",
      "Epoch 47, loss: 2.304218\n",
      "Epoch 48, loss: 2.304240\n",
      "Epoch 49, loss: 2.304262\n",
      "Epoch 50, loss: 2.304284\n",
      "Epoch 51, loss: 2.304306\n",
      "Epoch 52, loss: 2.304329\n",
      "Epoch 53, loss: 2.304351\n",
      "Epoch 54, loss: 2.304374\n",
      "Epoch 55, loss: 2.304397\n",
      "Epoch 56, loss: 2.304420\n",
      "Epoch 57, loss: 2.304443\n",
      "Epoch 58, loss: 2.304467\n",
      "Epoch 59, loss: 2.304490\n",
      "Epoch 60, loss: 2.304514\n",
      "Epoch 61, loss: 2.304538\n",
      "Epoch 62, loss: 2.304561\n",
      "Epoch 63, loss: 2.304586\n",
      "Epoch 64, loss: 2.304610\n",
      "Epoch 65, loss: 2.304634\n",
      "Epoch 66, loss: 2.304659\n",
      "Epoch 67, loss: 2.304683\n",
      "Epoch 68, loss: 2.304708\n",
      "Epoch 69, loss: 2.304733\n",
      "Epoch 70, loss: 2.304758\n",
      "Epoch 71, loss: 2.304784\n",
      "Epoch 72, loss: 2.304809\n",
      "Epoch 73, loss: 2.304834\n",
      "Epoch 74, loss: 2.304860\n",
      "Epoch 75, loss: 2.304886\n",
      "Epoch 76, loss: 2.304912\n",
      "Epoch 77, loss: 2.304938\n",
      "Epoch 78, loss: 2.304964\n",
      "Epoch 79, loss: 2.304991\n",
      "Epoch 80, loss: 2.305017\n",
      "Epoch 81, loss: 2.305044\n",
      "Epoch 82, loss: 2.305071\n",
      "Epoch 83, loss: 2.305098\n",
      "Epoch 84, loss: 2.305125\n",
      "Epoch 85, loss: 2.305152\n",
      "Epoch 86, loss: 2.305180\n",
      "Epoch 87, loss: 2.305207\n",
      "Epoch 88, loss: 2.305235\n",
      "Epoch 89, loss: 2.305263\n",
      "Epoch 90, loss: 2.305291\n",
      "Epoch 91, loss: 2.305319\n",
      "Epoch 92, loss: 2.305348\n",
      "Epoch 93, loss: 2.305376\n",
      "Epoch 94, loss: 2.305405\n",
      "Epoch 95, loss: 2.305433\n",
      "Epoch 96, loss: 2.305462\n",
      "Epoch 97, loss: 2.305491\n",
      "Epoch 98, loss: 2.305520\n",
      "Epoch 99, loss: 2.305550\n",
      "Accuracy after training for 100 epochs:  0.051\n",
      "Epoch 0, loss: 2.302885\n",
      "Epoch 1, loss: 2.302887\n",
      "Epoch 2, loss: 2.302890\n",
      "Epoch 3, loss: 2.302893\n",
      "Epoch 4, loss: 2.302896\n",
      "Epoch 5, loss: 2.302899\n",
      "Epoch 6, loss: 2.302903\n",
      "Epoch 7, loss: 2.302906\n",
      "Epoch 8, loss: 2.302909\n",
      "Epoch 9, loss: 2.302912\n",
      "Epoch 10, loss: 2.302915\n",
      "Epoch 11, loss: 2.302918\n",
      "Epoch 12, loss: 2.302921\n",
      "Epoch 13, loss: 2.302924\n",
      "Epoch 14, loss: 2.302927\n",
      "Epoch 15, loss: 2.302930\n",
      "Epoch 16, loss: 2.302934\n",
      "Epoch 17, loss: 2.302937\n",
      "Epoch 18, loss: 2.302940\n",
      "Epoch 19, loss: 2.302943\n",
      "Epoch 20, loss: 2.302947\n",
      "Epoch 21, loss: 2.302950\n",
      "Epoch 22, loss: 2.302953\n",
      "Epoch 23, loss: 2.302956\n",
      "Epoch 24, loss: 2.302960\n",
      "Epoch 25, loss: 2.302963\n",
      "Epoch 26, loss: 2.302966\n",
      "Epoch 27, loss: 2.302970\n",
      "Epoch 28, loss: 2.302973\n",
      "Epoch 29, loss: 2.302976\n",
      "Epoch 30, loss: 2.302980\n",
      "Epoch 31, loss: 2.302983\n",
      "Epoch 32, loss: 2.302987\n",
      "Epoch 33, loss: 2.302990\n",
      "Epoch 34, loss: 2.302993\n",
      "Epoch 35, loss: 2.302997\n",
      "Epoch 36, loss: 2.303000\n",
      "Epoch 37, loss: 2.303004\n",
      "Epoch 38, loss: 2.303007\n",
      "Epoch 39, loss: 2.303011\n",
      "Epoch 40, loss: 2.303014\n",
      "Epoch 41, loss: 2.303018\n",
      "Epoch 42, loss: 2.303022\n",
      "Epoch 43, loss: 2.303025\n",
      "Epoch 44, loss: 2.303029\n",
      "Epoch 45, loss: 2.303032\n",
      "Epoch 46, loss: 2.303036\n",
      "Epoch 47, loss: 2.303040\n",
      "Epoch 48, loss: 2.303043\n",
      "Epoch 49, loss: 2.303047\n",
      "Epoch 50, loss: 2.303051\n",
      "Epoch 51, loss: 2.303054\n",
      "Epoch 52, loss: 2.303058\n",
      "Epoch 53, loss: 2.303062\n",
      "Epoch 54, loss: 2.303065\n",
      "Epoch 55, loss: 2.303069\n",
      "Epoch 56, loss: 2.303073\n",
      "Epoch 57, loss: 2.303077\n",
      "Epoch 58, loss: 2.303080\n",
      "Epoch 59, loss: 2.303084\n",
      "Epoch 60, loss: 2.303088\n",
      "Epoch 61, loss: 2.303092\n",
      "Epoch 62, loss: 2.303096\n",
      "Epoch 63, loss: 2.303100\n",
      "Epoch 64, loss: 2.303104\n",
      "Epoch 65, loss: 2.303107\n",
      "Epoch 66, loss: 2.303111\n",
      "Epoch 67, loss: 2.303115\n",
      "Epoch 68, loss: 2.303119\n",
      "Epoch 69, loss: 2.303123\n",
      "Epoch 70, loss: 2.303127\n",
      "Epoch 71, loss: 2.303131\n",
      "Epoch 72, loss: 2.303135\n",
      "Epoch 73, loss: 2.303139\n",
      "Epoch 74, loss: 2.303143\n",
      "Epoch 75, loss: 2.303147\n",
      "Epoch 76, loss: 2.303151\n",
      "Epoch 77, loss: 2.303155\n",
      "Epoch 78, loss: 2.303159\n",
      "Epoch 79, loss: 2.303164\n",
      "Epoch 80, loss: 2.303168\n",
      "Epoch 81, loss: 2.303172\n",
      "Epoch 82, loss: 2.303176\n",
      "Epoch 83, loss: 2.303180\n",
      "Epoch 84, loss: 2.303184\n",
      "Epoch 85, loss: 2.303188\n",
      "Epoch 86, loss: 2.303193\n",
      "Epoch 87, loss: 2.303197\n",
      "Epoch 88, loss: 2.303201\n",
      "Epoch 89, loss: 2.303205\n",
      "Epoch 90, loss: 2.303210\n",
      "Epoch 91, loss: 2.303214\n",
      "Epoch 92, loss: 2.303218\n",
      "Epoch 93, loss: 2.303222\n",
      "Epoch 94, loss: 2.303227\n",
      "Epoch 95, loss: 2.303231\n",
      "Epoch 96, loss: 2.303235\n",
      "Epoch 97, loss: 2.303240\n",
      "Epoch 98, loss: 2.303244\n",
      "Epoch 99, loss: 2.303249\n",
      "Accuracy after training for 100 epochs:  0.051\n",
      "Epoch 0, loss: 2.368996\n",
      "Epoch 1, loss: 2.369040\n",
      "Epoch 2, loss: 2.369084\n",
      "Epoch 3, loss: 2.369128\n",
      "Epoch 4, loss: 2.369172\n",
      "Epoch 5, loss: 2.369216\n",
      "Epoch 6, loss: 2.369260\n",
      "Epoch 7, loss: 2.369304\n",
      "Epoch 8, loss: 2.369349\n",
      "Epoch 9, loss: 2.369393\n",
      "Epoch 10, loss: 2.369437\n",
      "Epoch 11, loss: 2.369481\n",
      "Epoch 12, loss: 2.369525\n",
      "Epoch 13, loss: 2.369569\n",
      "Epoch 14, loss: 2.369613\n",
      "Epoch 15, loss: 2.369657\n",
      "Epoch 16, loss: 2.369701\n",
      "Epoch 17, loss: 2.369746\n",
      "Epoch 18, loss: 2.369790\n",
      "Epoch 19, loss: 2.369834\n",
      "Epoch 20, loss: 2.369878\n",
      "Epoch 21, loss: 2.369922\n",
      "Epoch 22, loss: 2.369967\n",
      "Epoch 23, loss: 2.370011\n",
      "Epoch 24, loss: 2.370055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, loss: 2.370100\n",
      "Epoch 26, loss: 2.370144\n",
      "Epoch 27, loss: 2.370188\n",
      "Epoch 28, loss: 2.370233\n",
      "Epoch 29, loss: 2.370277\n",
      "Epoch 30, loss: 2.370321\n",
      "Epoch 31, loss: 2.370366\n",
      "Epoch 32, loss: 2.370410\n",
      "Epoch 33, loss: 2.370454\n",
      "Epoch 34, loss: 2.370499\n",
      "Epoch 35, loss: 2.370543\n",
      "Epoch 36, loss: 2.370588\n",
      "Epoch 37, loss: 2.370632\n",
      "Epoch 38, loss: 2.370677\n",
      "Epoch 39, loss: 2.370721\n",
      "Epoch 40, loss: 2.370766\n",
      "Epoch 41, loss: 2.370810\n",
      "Epoch 42, loss: 2.370855\n",
      "Epoch 43, loss: 2.370899\n",
      "Epoch 44, loss: 2.370944\n",
      "Epoch 45, loss: 2.370989\n",
      "Epoch 46, loss: 2.371033\n",
      "Epoch 47, loss: 2.371078\n",
      "Epoch 48, loss: 2.371122\n",
      "Epoch 49, loss: 2.371167\n",
      "Epoch 50, loss: 2.371212\n",
      "Epoch 51, loss: 2.371256\n",
      "Epoch 52, loss: 2.371301\n",
      "Epoch 53, loss: 2.371346\n",
      "Epoch 54, loss: 2.371391\n",
      "Epoch 55, loss: 2.371435\n",
      "Epoch 56, loss: 2.371480\n",
      "Epoch 57, loss: 2.371525\n",
      "Epoch 58, loss: 2.371570\n",
      "Epoch 59, loss: 2.371614\n",
      "Epoch 60, loss: 2.371659\n",
      "Epoch 61, loss: 2.371704\n",
      "Epoch 62, loss: 2.371749\n",
      "Epoch 63, loss: 2.371794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-39732755aa58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreg_strengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/opn/CS/csc/dlcourse_ai/assignments/assignment1/linear_classifer_solution.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# and regularization!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mloss_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mloss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier.fit(train_X, train_y, epochs=100, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(\"Accuracy after training for 100 epochs: \", accuracy)\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
