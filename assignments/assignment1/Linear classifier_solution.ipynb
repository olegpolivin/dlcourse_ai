{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics_solution import multiclass_accuracy \n",
    "import linear_classifer_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer_solution.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer_solution.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer_solution.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer_solution.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer_solution.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradients are different at (0, 2). Analytic: -0.32202, Numeric: -0.09980\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer_solution.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer_solution.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer_solution.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer_solution.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer_solution.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer_solution.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397150\n",
      "Epoch 1, loss: 2.330214\n",
      "Epoch 2, loss: 2.310786\n",
      "Epoch 3, loss: 2.303960\n",
      "Epoch 4, loss: 2.303279\n",
      "Epoch 5, loss: 2.302914\n",
      "Epoch 6, loss: 2.302568\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301254\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer_solution.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10,  learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121177550>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.126\n",
      "Epoch 0, loss: 2.301652\n",
      "Epoch 1, loss: 2.301755\n",
      "Epoch 2, loss: 2.302310\n",
      "Epoch 3, loss: 2.303148\n",
      "Epoch 4, loss: 2.302006\n",
      "Epoch 5, loss: 2.301725\n",
      "Epoch 6, loss: 2.302348\n",
      "Epoch 7, loss: 2.301949\n",
      "Epoch 8, loss: 2.301954\n",
      "Epoch 9, loss: 2.302569\n",
      "Epoch 10, loss: 2.301801\n",
      "Epoch 11, loss: 2.301840\n",
      "Epoch 12, loss: 2.302846\n",
      "Epoch 13, loss: 2.301958\n",
      "Epoch 14, loss: 2.301105\n",
      "Epoch 15, loss: 2.302637\n",
      "Epoch 16, loss: 2.301467\n",
      "Epoch 17, loss: 2.302400\n",
      "Epoch 18, loss: 2.301496\n",
      "Epoch 19, loss: 2.303081\n",
      "Epoch 20, loss: 2.302153\n",
      "Epoch 21, loss: 2.301707\n",
      "Epoch 22, loss: 2.301216\n",
      "Epoch 23, loss: 2.302376\n",
      "Epoch 24, loss: 2.301831\n",
      "Epoch 25, loss: 2.301443\n",
      "Epoch 26, loss: 2.301235\n",
      "Epoch 27, loss: 2.302408\n",
      "Epoch 28, loss: 2.301243\n",
      "Epoch 29, loss: 2.301511\n",
      "Epoch 30, loss: 2.302070\n",
      "Epoch 31, loss: 2.301456\n",
      "Epoch 32, loss: 2.302713\n",
      "Epoch 33, loss: 2.301880\n",
      "Epoch 34, loss: 2.300800\n",
      "Epoch 35, loss: 2.301292\n",
      "Epoch 36, loss: 2.301689\n",
      "Epoch 37, loss: 2.302266\n",
      "Epoch 38, loss: 2.301072\n",
      "Epoch 39, loss: 2.301708\n",
      "Epoch 40, loss: 2.301506\n",
      "Epoch 41, loss: 2.302000\n",
      "Epoch 42, loss: 2.302631\n",
      "Epoch 43, loss: 2.301327\n",
      "Epoch 44, loss: 2.301179\n",
      "Epoch 45, loss: 2.302589\n",
      "Epoch 46, loss: 2.301206\n",
      "Epoch 47, loss: 2.301450\n",
      "Epoch 48, loss: 2.301949\n",
      "Epoch 49, loss: 2.300993\n",
      "Epoch 50, loss: 2.301867\n",
      "Epoch 51, loss: 2.301890\n",
      "Epoch 52, loss: 2.301891\n",
      "Epoch 53, loss: 2.301940\n",
      "Epoch 54, loss: 2.301943\n",
      "Epoch 55, loss: 2.301836\n",
      "Epoch 56, loss: 2.302296\n",
      "Epoch 57, loss: 2.302120\n",
      "Epoch 58, loss: 2.301099\n",
      "Epoch 59, loss: 2.301467\n",
      "Epoch 60, loss: 2.302126\n",
      "Epoch 61, loss: 2.302279\n",
      "Epoch 62, loss: 2.302110\n",
      "Epoch 63, loss: 2.301425\n",
      "Epoch 64, loss: 2.301302\n",
      "Epoch 65, loss: 2.302114\n",
      "Epoch 66, loss: 2.301173\n",
      "Epoch 67, loss: 2.301739\n",
      "Epoch 68, loss: 2.302085\n",
      "Epoch 69, loss: 2.301888\n",
      "Epoch 70, loss: 2.302002\n",
      "Epoch 71, loss: 2.302720\n",
      "Epoch 72, loss: 2.302177\n",
      "Epoch 73, loss: 2.301694\n",
      "Epoch 74, loss: 2.301847\n",
      "Epoch 75, loss: 2.302598\n",
      "Epoch 76, loss: 2.301427\n",
      "Epoch 77, loss: 2.302765\n",
      "Epoch 78, loss: 2.302958\n",
      "Epoch 79, loss: 2.301959\n",
      "Accuracy after training for 100 epochs:  0.119\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with learning rate = 1 and regularization strength = 0\n",
      "Accuracy after training for 200 epochs:  0.163 \n",
      "\n",
      "Fitting with learning rate = 1 and regularization strength = 0.1\n",
      "Accuracy after training for 200 epochs:  0.144 \n",
      "\n",
      "Fitting with learning rate = 1 and regularization strength = 0.01\n",
      "Accuracy after training for 200 epochs:  0.157 \n",
      "\n",
      "Fitting with learning rate = 1 and regularization strength = 0.001\n",
      "Accuracy after training for 200 epochs:  0.135 \n",
      "\n",
      "Fitting with learning rate = 1 and regularization strength = 0.0001\n",
      "Accuracy after training for 200 epochs:  0.154 \n",
      "\n",
      "Fitting with learning rate = 0.1 and regularization strength = 0\n",
      "Accuracy after training for 200 epochs:  0.241 \n",
      "\n",
      "Fitting with learning rate = 0.1 and regularization strength = 0.1\n",
      "Accuracy after training for 200 epochs:  0.204 \n",
      "\n",
      "Fitting with learning rate = 0.1 and regularization strength = 0.01\n",
      "Accuracy after training for 200 epochs:  0.235 \n",
      "\n",
      "Fitting with learning rate = 0.1 and regularization strength = 0.001\n",
      "Accuracy after training for 200 epochs:  0.247 \n",
      "\n",
      "Fitting with learning rate = 0.1 and regularization strength = 0.0001\n",
      "Accuracy after training for 200 epochs:  0.24 \n",
      "\n",
      "Fitting with learning rate = 0.01 and regularization strength = 0\n",
      "Accuracy after training for 200 epochs:  0.241 \n",
      "\n",
      "Fitting with learning rate = 0.01 and regularization strength = 0.1\n",
      "Accuracy after training for 200 epochs:  0.231 \n",
      "\n",
      "Fitting with learning rate = 0.01 and regularization strength = 0.01\n",
      "Accuracy after training for 200 epochs:  0.239 \n",
      "\n",
      "Fitting with learning rate = 0.01 and regularization strength = 0.001\n",
      "Accuracy after training for 200 epochs:  0.24 \n",
      "\n",
      "Fitting with learning rate = 0.01 and regularization strength = 0.0001\n",
      "Accuracy after training for 200 epochs:  0.241 \n",
      "\n",
      "Fitting with learning rate = 0.001 and regularization strength = 0\n",
      "Accuracy after training for 200 epochs:  0.226 \n",
      "\n",
      "Fitting with learning rate = 0.001 and regularization strength = 0.1\n",
      "Accuracy after training for 200 epochs:  0.223 \n",
      "\n",
      "Fitting with learning rate = 0.001 and regularization strength = 0.01\n",
      "Accuracy after training for 200 epochs:  0.227 \n",
      "\n",
      "Fitting with learning rate = 0.001 and regularization strength = 0.001\n",
      "Accuracy after training for 200 epochs:  0.226 \n",
      "\n",
      "Fitting with learning rate = 0.001 and regularization strength = 0.0001\n",
      "Accuracy after training for 200 epochs:  0.226 \n",
      "\n",
      "Fitting with learning rate = 0.0001 and regularization strength = 0\n",
      "Accuracy after training for 200 epochs:  0.168 \n",
      "\n",
      "Fitting with learning rate = 0.0001 and regularization strength = 0.1\n",
      "Accuracy after training for 200 epochs:  0.168 \n",
      "\n",
      "Fitting with learning rate = 0.0001 and regularization strength = 0.01\n",
      "Accuracy after training for 200 epochs:  0.168 \n",
      "\n",
      "Fitting with learning rate = 0.0001 and regularization strength = 0.001\n",
      "Accuracy after training for 200 epochs:  0.168 \n",
      "\n",
      "Fitting with learning rate = 0.0001 and regularization strength = 0.0001\n",
      "Accuracy after training for 200 epochs:  0.168 \n",
      "\n",
      "best validation accuracy achieved: 0.247000\n",
      "best validation was achieved with lr = 0.1 and reg = 0.001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "reg_strengths = [0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer_solution.LinearSoftmaxClassifier()\n",
    "        print(f'Fitting with learning rate = {lr} and regularization strength = {reg}')\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(f\"Accuracy after training for {num_epochs} epochs: \", accuracy, \"\\n\")\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr = lr\n",
    "            best_reg = reg\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print(f'best validation was achieved with lr = {best_lr} and reg = {best_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.210000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
