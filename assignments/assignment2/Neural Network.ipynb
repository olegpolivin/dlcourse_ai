{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_0\n",
      "Gradient check passed!\n",
      "Checking gradient for B_0\n",
      "Gradient check passed!\n",
      "Checking gradient for W_2\n",
      "Gradient check passed!\n",
      "Checking gradient for B_2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_0\n",
      "Gradient check passed!\n",
      "Checking gradient for B_0\n",
      "Gradient check passed!\n",
      "Checking gradient for W_2\n",
      "Gradient check passed!\n",
      "Checking gradient for B_2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss_with_reg, loss)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.257349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.156543, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.284460, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.133622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237598, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.009313, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.123666, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226096, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.098399, Train accuracy: 0.202667, val accuracy: 0.210000\n",
      "Loss: 2.086190, Train accuracy: 0.227778, val accuracy: 0.237000\n",
      "Loss: 2.089915, Train accuracy: 0.250778, val accuracy: 0.251000\n",
      "Loss: 2.184890, Train accuracy: 0.264889, val accuracy: 0.265000\n",
      "Loss: 2.132385, Train accuracy: 0.286889, val accuracy: 0.285000\n",
      "Loss: 2.004860, Train accuracy: 0.321778, val accuracy: 0.328000\n",
      "Loss: 1.948629, Train accuracy: 0.359778, val accuracy: 0.364000\n",
      "Loss: 1.650998, Train accuracy: 0.391000, val accuracy: 0.383000\n",
      "Loss: 1.944239, Train accuracy: 0.419444, val accuracy: 0.402000\n",
      "Loss: 1.755136, Train accuracy: 0.449111, val accuracy: 0.434000\n",
      "Loss: 1.637437, Train accuracy: 0.473333, val accuracy: 0.458000\n",
      "Loss: 1.478885, Train accuracy: 0.486111, val accuracy: 0.467000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124eccd90>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12547ded0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history, label='Train accuracy')\n",
    "plt.plot(val_history, label='Val accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.282786, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234582, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.145585, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196805, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287513, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248047, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.186539, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.912230, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256953, Train accuracy: 0.200556, val accuracy: 0.210000\n",
      "Loss: 1.962076, Train accuracy: 0.222889, val accuracy: 0.232000\n",
      "Loss: 1.949703, Train accuracy: 0.247222, val accuracy: 0.247000\n",
      "Loss: 2.092175, Train accuracy: 0.263444, val accuracy: 0.264000\n",
      "Loss: 1.871600, Train accuracy: 0.275111, val accuracy: 0.277000\n",
      "Loss: 1.807875, Train accuracy: 0.289778, val accuracy: 0.296000\n",
      "Loss: 1.756844, Train accuracy: 0.318333, val accuracy: 0.325000\n",
      "Loss: 1.824883, Train accuracy: 0.346667, val accuracy: 0.354000\n",
      "Loss: 1.744549, Train accuracy: 0.382667, val accuracy: 0.381000\n",
      "Loss: 1.768013, Train accuracy: 0.404778, val accuracy: 0.396000\n",
      "Loss: 1.550735, Train accuracy: 0.431556, val accuracy: 0.428000\n",
      "Loss: 1.865711, Train accuracy: 0.452667, val accuracy: 0.447000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.477746, Train accuracy: 0.207111, val accuracy: 0.218000\n",
      "Loss: 1.670014, Train accuracy: 0.377333, val accuracy: 0.381000\n",
      "Loss: 1.544592, Train accuracy: 0.542556, val accuracy: 0.545000\n",
      "Loss: 1.194426, Train accuracy: 0.619333, val accuracy: 0.606000\n",
      "Loss: 1.423355, Train accuracy: 0.669667, val accuracy: 0.641000\n",
      "Loss: 1.276991, Train accuracy: 0.686889, val accuracy: 0.669000\n",
      "Loss: 1.089824, Train accuracy: 0.732444, val accuracy: 0.689000\n",
      "Loss: 0.956152, Train accuracy: 0.745333, val accuracy: 0.704000\n",
      "Loss: 0.683732, Train accuracy: 0.736667, val accuracy: 0.702000\n",
      "Loss: 0.457056, Train accuracy: 0.760444, val accuracy: 0.704000\n",
      "Loss: 0.352953, Train accuracy: 0.785333, val accuracy: 0.716000\n",
      "Loss: 0.617933, Train accuracy: 0.788889, val accuracy: 0.719000\n",
      "Loss: 0.984979, Train accuracy: 0.787000, val accuracy: 0.709000\n",
      "Loss: 0.395877, Train accuracy: 0.812667, val accuracy: 0.717000\n",
      "Loss: 0.551306, Train accuracy: 0.820222, val accuracy: 0.728000\n",
      "Loss: 0.574415, Train accuracy: 0.837444, val accuracy: 0.735000\n",
      "Loss: 0.651523, Train accuracy: 0.834667, val accuracy: 0.740000\n",
      "Loss: 0.469890, Train accuracy: 0.822111, val accuracy: 0.734000\n",
      "Loss: 0.719930, Train accuracy: 0.828556, val accuracy: 0.711000\n",
      "Loss: 0.582533, Train accuracy: 0.867333, val accuracy: 0.753000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.335231, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.335398, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.295406, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.281863, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.321520, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.309492, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.258361, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.284447, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.118838, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.894776, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.841060, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.200363, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.900459, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.350083, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.765621, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.058465, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.656963, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.164954, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.657344, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.347882, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.693521, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.145503, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.513284, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.985874, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.983627, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.553925, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.014392, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.035584, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.515096, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.013821, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.651064, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.984239, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.678077, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.779123, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.649719, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.599322, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.479175, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.161599, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.567347, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.812854, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.823061, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.008352, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.079440, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.555739, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.474699, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.366782, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.302327, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.619285, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.661670, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.954083, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.417140, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.239371, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.667747, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.708231, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.535356, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.480927, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.369449, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.722858, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.602700, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.223797, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.242618, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.619644, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.675545, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.910951, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.005273, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.206812, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.895029, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.254798, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.948347, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.603178, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.612660, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.567572, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.338671, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.677150, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.186417, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.237661, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.565372, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.034595, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.270859, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.606872, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.590533, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.294420, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.417491, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.382075, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.270893, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.203885, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.352186, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.947821, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.528860, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.671353, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.246595, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.782472, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.590990, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.005921, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.652412, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.369474, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.331557, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.296878, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.130422, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.571685, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.436358, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.998381, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.535568, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.488000, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.194673, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.731415, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.300332, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.375145, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.327631, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.172868, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.342957, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.453401, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496351, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.340585, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.291423, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.206443, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.422890, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.206947, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.203456, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.176978, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.376045, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.095767, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.344160, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.555067, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.422940, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.214594, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.116422, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.014563, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.568630, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.277445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.455414, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.301567, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.490663, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.126107, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.272208, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.326268, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.212037, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.204702, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.132987, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.196286, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.344779, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.385094, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.405613, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.386572, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.455443, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.506528, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.128289, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.279278, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.353710, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.394760, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.330895, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.254492, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.134727, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.250895, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.747145, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.728131, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.641526, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 1.654881, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.000118, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.303145, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.371834, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.206700, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.251464, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 0.635919, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.706850, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.927683, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.144015, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.421437, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.268370, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.281188, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=0.33, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea will be the following: first, we choose among large range of parameters, and then I will adjust further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: learning rate = 0.01, regularization strength = 1e-05, learning rate decay = 0.9, hidden_layer_size = 100\n",
      "Resulting accuracy is 0.639\n",
      "\n",
      "Fitting: learning rate = 0.01, regularization strength = 1e-05, learning rate decay = 0.9, hidden_layer_size = 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-a5cbbb217aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fitting: learning rate = {lr}, regularization strength = {reg}, learning rate decay = {lrd}, hidden_layer_size = {hidden_layer_s}'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mcoef_to_val_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Resulting accuracy is {val_history[-1]}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/storytelling/ML/Courses/cs/CSC/S2/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             train_accuracy = self.compute_accuracy(self.dataset.train_X,\n\u001b[0;32m--> 119\u001b[0;31m                                                    self.dataset.train_y)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             val_accuracy = self.compute_accuracy(self.dataset.val_X,\n",
      "\u001b[0;32m~/Desktop/storytelling/ML/Courses/cs/CSC/S2/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/storytelling/ML/Courses/cs/CSC/S2/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m           \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/storytelling/ML/Courses/cs/CSC/S2/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-2, 1e-1, 0.35]\n",
    "reg_strength = [1e-5, 1e-4]\n",
    "learning_rate_decay = [0.9, 0.99, 0.999]\n",
    "hidden_layer_size = [100, 200, 300]\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = -1\n",
    "coef_to_val_accuracy = {}\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strength:\n",
    "        for lrd in learning_rate_decay:\n",
    "            for hidden_layer_s in hidden_layer_size:\n",
    "                model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_s, reg = reg)\n",
    "                trainer = Trainer(model,\n",
    "                                  dataset,\n",
    "                                  MomentumSGD(),\n",
    "                                  num_epochs = num_epochs,\n",
    "                                  batch_size = batch_size,\n",
    "                                  learning_rate=lr,\n",
    "                                  learning_rate_decay=lrd)\n",
    "                print(f'Fitting: learning rate = {lr}, regularization strength = {reg}, learning rate decay = {lrd}, hidden_layer_size = {hidden_layer_s}' )\n",
    "\n",
    "                loss_history, train_history, val_history = trainer.fit()\n",
    "                coef_to_val_accuracy[(lr, reg, hidden_layer_s)] = val_history[-1]\n",
    "                print(f'Resulting accuracy is {val_history[-1]}\\n')\n",
    "                if val_history[-1] > best_val_accuracy:\n",
    "                    best_classifier = model\n",
    "                    best_val_accuracy = val_history[-1]\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
